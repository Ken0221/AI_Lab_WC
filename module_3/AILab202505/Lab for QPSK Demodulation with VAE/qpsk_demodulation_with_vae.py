# -*- coding: utf-8 -*-
"""QPSK_Demodulation_with_VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1isNy8CH35QNDFnrABYKzKzLHo7Pfryqz
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Load Data

import scipy
# from scipy import io
import numpy as np
# from numpy import expand_dims
# import os
# from PIL import Image
# import matplotlib.pyplot as plt
from sklearn import preprocessing


class Dataset1:
    def __init__(self, prob=1):
        self.prob = prob

        self.signals = self.load_signals()
        self.index = self.load_index()

    def batches(self, batch_size=32):

        idx = np.arange(self.signals.shape[0])
        np.random.shuffle(idx)
        self.signals = self.signals[idx]
        self.index = self.index[idx]

        n_batches = self.signals.shape[0] // batch_size
        for ii in range(0, int(n_batches * self.prob)):
            x = self.signals[ii * batch_size:(ii + 1) * batch_size]
            y = self.index[ii * batch_size:(ii + 1) * batch_size]
            
            yield x, y


    def load_signals(self):
        # train_dataset_struct = scipy.io.loadmat('/content/drive/My Drive/Train_Signals/Input_qpsk_signal.mat')
        train_dataset_struct = scipy.io.loadmat('./Train_Signals/Input_qpsk_radar_signal.mat') 
        train_dataset = train_dataset_struct['qpsk_set']
        training_set = np.asarray(train_dataset, dtype = 'float32')
        training_set = preprocessing.scale(training_set)
        print(training_set.shape)
        return training_set

    def load_index(self):
        # train_index_struct = scipy.io.loadmat('/content/drive/My Drive/Train_Signals/Truth_index.mat')
        train_index_struct = scipy.io.loadmat('./Train_Signals/Truth_radar_index.mat')
        train_index = train_index_struct['symbol_index']
        index_set = np.asarray(train_index, dtype = 'float32')
        print(index_set.shape)
        return index_set


class Dataset2:
    def __init__(self, prob=1):
        self.prob = prob

        self.signals = self.load_signals()
        self.index = self.load_index()

    def batches(self, batch_size=100):

        idx = np.arange(self.signals.shape[0])
        np.random.shuffle(idx)
        self.signals = self.signals[idx]
        self.index = self.index[idx]

        n_batches = self.signals.shape[0] // batch_size
        for ii in range(0, int(n_batches * self.prob)):
            x = self.signals[ii * batch_size:(ii + 1) * batch_size]
            y = self.index[ii * batch_size:(ii + 1) * batch_size]
            
            yield x, y


    def load_signals(self):
        # test_dataset_struct = scipy.io.loadmat('/content/drive/My Drive/Test_Signals/Testing_qpsk_signal.mat')
        test_dataset_struct = scipy.io.loadmat('./Test_Signals/Testing_qpsk_radar_signal.mat')
        test_dataset = test_dataset_struct['qpsk_set']
        testing_set = np.asarray(test_dataset, dtype = 'float32')
        testing_set = preprocessing.scale(testing_set)
        print(testing_set.shape)
        return testing_set

    def load_index(self):
        # test_index_struct = scipy.io.loadmat('/content/drive/My Drive/Test_Signals/Test_index.mat')
        test_index_struct = scipy.io.loadmat('./Test_Signals/Test_radar_index.mat')
        test_index = test_index_struct['symbol_index']
        index_set = np.asarray(test_index, dtype = 'float32')
        print(index_set.shape)
        return index_set

# VAE Architecture
# from IPython import display

# import glob
# import imageio
# import matplotlib.pyplot as plt
# import numpy as np
# import PIL
import tensorflow as tf
# import tensorflow_probability as tfp
import time

class CVAE(tf.keras.Model):
  """Convolutional variational autoencoder."""

  def __init__(self, latent_dim):
    super(CVAE, self).__init__()
    self.latent_dim = latent_dim
    self.encoder = tf.keras.Sequential(
        [
            # tf.keras.layers.InputLayer(input_shape=(3001,)),
         
            # tf.keras.layers.Dense(1024),
            # tf.keras.layers.Dense(512),
            # tf.keras.layers.Dense(256),
         
            tf.keras.layers.InputLayer(input_shape=(2200,)),
         
            tf.keras.layers.Dense(512),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(128),
            
            tf.keras.layers.Dense(latent_dim + latent_dim),
        ]
    )

    self.decoder = tf.keras.Sequential(
        [
            # tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
            # tf.keras.layers.Dense(256),
            # tf.keras.layers.Dense(512),
            # tf.keras.layers.Dense(1024),
            # tf.keras.layers.Dense(3001),
            
            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
            tf.keras.layers.Dense(128),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(512),
            tf.keras.layers.Dense(2200),

            tf.keras.layers.Dense(4),
        ]
    )

  @tf.function
  def testing(self, x):
    temp = self.encoder(x)
    return temp

  def sample(self, eps=None):
    if eps is None:
      eps = tf.random.normal(shape=(100, self.latent_dim))
    return self.decode(eps, apply_sigmoid=True)

  def encode(self, x):
    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
    return mean, logvar

  def reparameterize(self, mean, logvar):
    eps = tf.random.normal(shape=mean.shape)
    return eps * tf.exp(logvar * .5) + mean

  def decode(self, z, apply_sigmoid=False):
    logits = self.decoder(z)
    if apply_sigmoid:
      probs = tf.sigmoid(logits)
      return probs
    return logits

latent_dim = 100
train_dataset = Dataset1(prob=1)
test_dataset = Dataset2(prob=1)
Demodulator = CVAE(latent_dim)

optimizer = tf.keras.optimizers.Adam(1e-5)

def compute_loss(model, x, y):
  # test = model.testing(x)
  # print(test.shape)
  # print(test)
  mean, logvar = model.encode(x)
  z = model.reparameterize(mean, logvar)
  x_logit = model.decode(z, apply_sigmoid=False) # Output Signal
  # print(x_logit)
  # print(y)
  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y))

  return loss


@tf.function
def train_step(Demodulator, x, y, optimizer):
  with tf.GradientTape() as tape:
    loss = compute_loss(Demodulator, x, y)
  gradients = tape.gradient(loss, Demodulator.trainable_variables)
  optimizer.apply_gradients(zip(gradients, Demodulator.trainable_variables))

import matplotlib.pyplot as plt

test_sample = test_dataset.load_signals()
test_index = test_dataset.load_index()

batch_size = 32
epochs = 100

VAE_Loss = []

for epoch in range(1, epochs + 1):
  start_time = time.time()
  for x, y in train_dataset.batches(batch_size):
    tf.config.run_functions_eagerly(True)
    train_step(Demodulator, x, y, optimizer)

  VAE_loss = compute_loss(Demodulator, x, y)
  VAE_Loss.append(VAE_loss)

  end_time = time.time()
  # display.clear_output(wait=False)

  print('Epoch: {}, time elapse for current epoch: {}, VAE_Loss: {}'
        .format(epoch, end_time - start_time, VAE_loss))
    

  if epoch % 100 == 0:    
    plt.figure()
    plt.plot(VAE_Loss)
    plt.xlabel('epoch')
    plt.ylabel('VAE_Loss')
    plt.show()

# Testing
def testing_process(model, test_sample, test_index, num_sample):
  mean, logvar = model.encode(test_sample)
  z = model.reparameterize(mean, logvar)
  predictions = model.decode(z, apply_sigmoid=True)
  predictions = predictions.numpy()
  # print(predictions)
  error_times = 0
  est_index = np.argmax(predictions, axis=1)
  truth_index = np.argmax(test_index, axis=1)
  N = predictions.shape[0]
  accuracy = (est_index == truth_index).sum() / N
  print('Prediction Accuracy = ', accuracy)

testing_process(Demodulator, test_sample, test_index, 100)

## predict test_sample[0,:]
example = 0
test_example = test_sample[example,:]
mean, logvar = Demodulator.encode(test_sample)
z = Demodulator.reparameterize(mean, logvar)
predictions = Demodulator.decode(z, apply_sigmoid=True)
predictions = predictions.numpy()
truth_index = np.argmax(test_index[example,:])
est_index = np.argmax(predictions[example,:])
print('truth_index: ',truth_index)
print('est_index: ',est_index)